{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuzammilAshraf09/DEMO/blob/main/Muz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URz3stvwYc3S",
        "outputId": "39d9d226-ee17-44c3-a27f-70f01e7e3934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.33-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.33-py3-none-any.whl (887 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.2/887.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.33 ultralytics-thop-2.0.11\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bbIuuPKZYr79",
        "outputId": "c12e7866-7627-4c50-db8f-af9096d5a614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.36)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Epoch 1/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.5231 - loss: 0.7042 - val_accuracy: 0.4444 - val_loss: 0.7030 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.5743 - loss: 0.6831 - val_accuracy: 0.4444 - val_loss: 0.6855 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.6735 - loss: 0.6349 - val_accuracy: 0.5926 - val_loss: 0.6321 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.5983 - loss: 0.6558 - val_accuracy: 0.5926 - val_loss: 0.6086 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.6261 - loss: 0.6210 - val_accuracy: 0.7778 - val_loss: 0.5901 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.7233 - loss: 0.5682 - val_accuracy: 0.8148 - val_loss: 0.5482 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.7922 - loss: 0.5061 - val_accuracy: 0.8148 - val_loss: 0.5254 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.7771 - loss: 0.5505 - val_accuracy: 0.8148 - val_loss: 0.5043 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.7732 - loss: 0.4781 - val_accuracy: 0.8519 - val_loss: 0.4784 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.8683 - loss: 0.4384 - val_accuracy: 0.8889 - val_loss: 0.4135 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.8721 - loss: 0.4034 - val_accuracy: 0.8889 - val_loss: 0.3963 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.8795 - loss: 0.3264 - val_accuracy: 0.6667 - val_loss: 0.5215 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.8883 - loss: 0.3579 - val_accuracy: 0.9259 - val_loss: 0.3612 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.9263 - loss: 0.3152 - val_accuracy: 0.8889 - val_loss: 0.3806 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.8851 - loss: 0.2735 - val_accuracy: 0.8889 - val_loss: 0.3441 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3s/step - accuracy: 0.9366 - loss: 0.2445 - val_accuracy: 0.8519 - val_loss: 0.3882 - learning_rate: 1.0000e-04\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8095 - loss: 0.4306\n",
            "Test Loss: 0.4306168258190155\n",
            "Test Accuracy: 0.8095238208770752\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 1 person, 1 bed, 218.9ms\n",
            "Speed: 3.1ms preprocess, 218.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 204.5ms\n",
            "Speed: 3.1ms preprocess, 204.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 217.1ms\n",
            "Speed: 3.1ms preprocess, 217.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 217.7ms\n",
            "Speed: 3.4ms preprocess, 217.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 213.8ms\n",
            "Speed: 3.6ms preprocess, 213.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 246.8ms\n",
            "Speed: 4.0ms preprocess, 246.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 714.0ms\n",
            "Speed: 10.1ms preprocess, 714.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 227.2ms\n",
            "Speed: 3.1ms preprocess, 227.2ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 232.4ms\n",
            "Speed: 5.0ms preprocess, 232.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 162.0ms\n",
            "Speed: 5.1ms preprocess, 162.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\n",
            "0: 384x640 1 person, 141.3ms\n",
            "Speed: 3.2ms preprocess, 141.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 131.4ms\n",
            "Speed: 4.3ms preprocess, 131.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 145.3ms\n",
            "Speed: 3.6ms preprocess, 145.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 137.0ms\n",
            "Speed: 3.5ms preprocess, 137.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 130.1ms\n",
            "Speed: 3.2ms preprocess, 130.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 128.6ms\n",
            "Speed: 5.0ms preprocess, 128.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 143.6ms\n",
            "Speed: 4.9ms preprocess, 143.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.4ms\n",
            "Speed: 3.2ms preprocess, 133.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 127.2ms\n",
            "Speed: 5.8ms preprocess, 127.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 146.4ms\n",
            "Speed: 4.3ms preprocess, 146.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 128.3ms\n",
            "Speed: 3.2ms preprocess, 128.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.5ms\n",
            "Speed: 3.1ms preprocess, 129.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 150.7ms\n",
            "Speed: 3.2ms preprocess, 150.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 138.0ms\n",
            "Speed: 3.8ms preprocess, 138.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 131.8ms\n",
            "Speed: 4.8ms preprocess, 131.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 135.7ms\n",
            "Speed: 3.5ms preprocess, 135.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.9ms\n",
            "Speed: 3.5ms preprocess, 133.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 167.2ms\n",
            "Speed: 4.7ms preprocess, 167.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 131.9ms\n",
            "Speed: 3.9ms preprocess, 131.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 149.7ms\n",
            "Speed: 7.5ms preprocess, 149.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 131.5ms\n",
            "Speed: 9.7ms preprocess, 131.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 148.8ms\n",
            "Speed: 3.4ms preprocess, 148.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 128.6ms\n",
            "Speed: 3.3ms preprocess, 128.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 126.8ms\n",
            "Speed: 6.6ms preprocess, 126.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.2ms\n",
            "Speed: 3.4ms preprocess, 139.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.6ms\n",
            "Speed: 4.6ms preprocess, 135.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 210.0ms\n",
            "Speed: 7.1ms preprocess, 210.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 211.0ms\n",
            "Speed: 3.2ms preprocess, 211.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 201.3ms\n",
            "Speed: 3.3ms preprocess, 201.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 238.1ms\n",
            "Speed: 4.3ms preprocess, 238.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 206.1ms\n",
            "Speed: 3.2ms preprocess, 206.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 218.4ms\n",
            "Speed: 4.5ms preprocess, 218.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "\n",
            "0: 384x640 1 person, 210.1ms\n",
            "Speed: 3.7ms preprocess, 210.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 384x640 1 person, 224.4ms\n",
            "Speed: 3.4ms preprocess, 224.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 211.3ms\n",
            "Speed: 3.3ms preprocess, 211.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 1 person, 234.3ms\n",
            "Speed: 3.7ms preprocess, 234.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\n",
            "0: 384x640 1 person, 213.5ms\n",
            "Speed: 3.2ms preprocess, 213.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\n",
            "0: 384x640 1 person, 140.9ms\n",
            "Speed: 2.8ms preprocess, 140.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 134.3ms\n",
            "Speed: 3.2ms preprocess, 134.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.9ms\n",
            "Speed: 3.3ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.1ms\n",
            "Speed: 3.2ms preprocess, 135.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.9ms\n",
            "Speed: 3.3ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 139.3ms\n",
            "Speed: 6.3ms preprocess, 139.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 126.7ms\n",
            "Speed: 3.0ms preprocess, 126.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 384x640 1 person, 139.3ms\n",
            "Speed: 4.6ms preprocess, 139.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 150.8ms\n",
            "Speed: 3.3ms preprocess, 150.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 129.2ms\n",
            "Speed: 3.3ms preprocess, 129.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\n",
            "0: 384x640 1 person, 139.6ms\n",
            "Speed: 4.7ms preprocess, 139.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 133.6ms\n",
            "Speed: 3.2ms preprocess, 133.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 131.4ms\n",
            "Speed: 3.3ms preprocess, 131.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 140.0ms\n",
            "Speed: 3.4ms preprocess, 140.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 140.6ms\n",
            "Speed: 3.4ms preprocess, 140.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 134.5ms\n",
            "Speed: 3.5ms preprocess, 134.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 160.1ms\n",
            "Speed: 3.3ms preprocess, 160.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 137.7ms\n",
            "Speed: 3.4ms preprocess, 137.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.7ms\n",
            "Speed: 4.9ms preprocess, 133.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 154.6ms\n",
            "Speed: 3.5ms preprocess, 154.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 142.2ms\n",
            "Speed: 3.4ms preprocess, 142.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 144.0ms\n",
            "Speed: 3.8ms preprocess, 144.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 147.9ms\n",
            "Speed: 5.9ms preprocess, 147.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 136.7ms\n",
            "Speed: 3.5ms preprocess, 136.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 143.4ms\n",
            "Speed: 3.3ms preprocess, 143.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 148.0ms\n",
            "Speed: 4.9ms preprocess, 148.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 134.8ms\n",
            "Speed: 8.2ms preprocess, 134.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 138.9ms\n",
            "Speed: 8.3ms preprocess, 138.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 145.7ms\n",
            "Speed: 3.2ms preprocess, 145.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 129.4ms\n",
            "Speed: 3.6ms preprocess, 129.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 142.1ms\n",
            "Speed: 3.4ms preprocess, 142.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 221.6ms\n",
            "Speed: 6.2ms preprocess, 221.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 201.4ms\n",
            "Speed: 6.5ms preprocess, 201.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 217.8ms\n",
            "Speed: 3.3ms preprocess, 217.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 219.8ms\n",
            "Speed: 5.1ms preprocess, 219.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 239.4ms\n",
            "Speed: 3.3ms preprocess, 239.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 202.9ms\n",
            "Speed: 3.4ms preprocess, 202.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 236.7ms\n",
            "Speed: 3.2ms preprocess, 236.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
            "\n",
            "0: 384x640 1 person, 238.4ms\n",
            "Speed: 9.0ms preprocess, 238.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 260.8ms\n",
            "Speed: 3.4ms preprocess, 260.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.6ms\n",
            "Speed: 3.7ms preprocess, 135.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.2ms\n",
            "Speed: 4.1ms preprocess, 133.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 143.1ms\n",
            "Speed: 3.3ms preprocess, 143.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.9ms\n",
            "Speed: 3.4ms preprocess, 139.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 130.2ms\n",
            "Speed: 4.2ms preprocess, 130.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 151.8ms\n",
            "Speed: 3.7ms preprocess, 151.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 137.9ms\n",
            "Speed: 3.2ms preprocess, 137.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 132.9ms\n",
            "Speed: 4.0ms preprocess, 132.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 152.2ms\n",
            "Speed: 3.3ms preprocess, 152.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 137.3ms\n",
            "Speed: 3.4ms preprocess, 137.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 138.5ms\n",
            "Speed: 3.5ms preprocess, 138.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 151.7ms\n",
            "Speed: 3.2ms preprocess, 151.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 138.0ms\n",
            "Speed: 3.8ms preprocess, 138.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 136.7ms\n",
            "Speed: 4.9ms preprocess, 136.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "\n",
            "0: 384x640 1 person, 143.3ms\n",
            "Speed: 4.5ms preprocess, 143.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 134.5ms\n",
            "Speed: 5.5ms preprocess, 134.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 145.0ms\n",
            "Speed: 3.3ms preprocess, 145.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 137.5ms\n",
            "Speed: 3.9ms preprocess, 137.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 384x640 1 person, 134.2ms\n",
            "Speed: 4.4ms preprocess, 134.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 129.3ms\n",
            "Speed: 2.9ms preprocess, 129.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 138.7ms\n",
            "Speed: 3.4ms preprocess, 138.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 149.6ms\n",
            "Speed: 3.2ms preprocess, 149.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 143.5ms\n",
            "Speed: 3.4ms preprocess, 143.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 133.0ms\n",
            "Speed: 3.9ms preprocess, 133.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "\n",
            "0: 384x640 1 person, 128.0ms\n",
            "Speed: 3.4ms preprocess, 128.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 136.3ms\n",
            "Speed: 3.4ms preprocess, 136.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 130.0ms\n",
            "Speed: 4.1ms preprocess, 130.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 162.1ms\n",
            "Speed: 6.0ms preprocess, 162.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 130.9ms\n",
            "Speed: 3.3ms preprocess, 130.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "\n",
            "0: 384x640 1 person, 139.0ms\n",
            "Speed: 3.5ms preprocess, 139.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 207.9ms\n",
            "Speed: 3.3ms preprocess, 207.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 1 person, 240.9ms\n",
            "Speed: 3.3ms preprocess, 240.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\n",
            "0: 384x640 1 person, 243.7ms\n",
            "Speed: 6.2ms preprocess, 243.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 1 person, 218.5ms\n",
            "Speed: 3.5ms preprocess, 218.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 384x640 1 person, 210.1ms\n",
            "Speed: 7.4ms preprocess, 210.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\n",
            "0: 384x640 1 person, 209.6ms\n",
            "Speed: 9.5ms preprocess, 209.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 1 person, 206.7ms\n",
            "Speed: 3.4ms preprocess, 206.7ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\n",
            "0: 384x640 1 person, 218.7ms\n",
            "Speed: 7.2ms preprocess, 218.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 227.6ms\n",
            "Speed: 5.6ms preprocess, 227.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
            "\n",
            "0: 384x640 1 person, 259.4ms\n",
            "Speed: 6.9ms preprocess, 259.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
            "\n",
            "0: 384x640 1 person, 134.5ms\n",
            "Speed: 3.5ms preprocess, 134.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 145.6ms\n",
            "Speed: 3.4ms preprocess, 145.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 384x640 1 person, 141.0ms\n",
            "Speed: 3.2ms preprocess, 141.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 133.0ms\n",
            "Speed: 3.2ms preprocess, 133.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 143.2ms\n",
            "Speed: 3.0ms preprocess, 143.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 140.5ms\n",
            "Speed: 3.5ms preprocess, 140.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 130.9ms\n",
            "Speed: 3.9ms preprocess, 130.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 138.1ms\n",
            "Speed: 5.9ms preprocess, 138.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 144.9ms\n",
            "Speed: 4.3ms preprocess, 144.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 132.3ms\n",
            "Speed: 4.7ms preprocess, 132.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 129.7ms\n",
            "Speed: 6.0ms preprocess, 129.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 154.1ms\n",
            "Speed: 7.8ms preprocess, 154.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.0ms\n",
            "Speed: 3.3ms preprocess, 133.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 128.7ms\n",
            "Speed: 3.7ms preprocess, 128.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 167.6ms\n",
            "Speed: 5.2ms preprocess, 167.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.8ms\n",
            "Speed: 3.4ms preprocess, 133.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 136.3ms\n",
            "Speed: 6.1ms preprocess, 136.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 153.2ms\n",
            "Speed: 4.1ms preprocess, 153.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 144.6ms\n",
            "Speed: 5.5ms preprocess, 144.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 140.7ms\n",
            "Speed: 3.5ms preprocess, 140.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 161.2ms\n",
            "Speed: 4.0ms preprocess, 161.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 129.4ms\n",
            "Speed: 6.6ms preprocess, 129.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 132.2ms\n",
            "Speed: 3.6ms preprocess, 132.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 143.7ms\n",
            "Speed: 3.3ms preprocess, 143.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 127.6ms\n",
            "Speed: 6.5ms preprocess, 127.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 127.8ms\n",
            "Speed: 7.8ms preprocess, 127.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 135.2ms\n",
            "Speed: 8.1ms preprocess, 135.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 128.8ms\n",
            "Speed: 3.6ms preprocess, 128.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 132.4ms\n",
            "Speed: 3.4ms preprocess, 132.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.8ms\n",
            "Speed: 4.0ms preprocess, 133.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 145.8ms\n",
            "Speed: 4.4ms preprocess, 145.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 214.9ms\n",
            "Speed: 3.4ms preprocess, 214.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 206.4ms\n",
            "Speed: 7.5ms preprocess, 206.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 206.0ms\n",
            "Speed: 7.5ms preprocess, 206.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 232.6ms\n",
            "Speed: 6.9ms preprocess, 232.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 217.3ms\n",
            "Speed: 5.4ms preprocess, 217.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 225.1ms\n",
            "Speed: 3.4ms preprocess, 225.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 212.5ms\n",
            "Speed: 5.1ms preprocess, 212.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 232.2ms\n",
            "Speed: 3.3ms preprocess, 232.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 226.6ms\n",
            "Speed: 3.8ms preprocess, 226.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 241.9ms\n",
            "Speed: 3.4ms preprocess, 241.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 2 persons, 2 beds, 228.8ms\n",
            "Speed: 3.8ms preprocess, 228.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 131.9ms\n",
            "Speed: 3.6ms preprocess, 131.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.6ms\n",
            "Speed: 6.8ms preprocess, 139.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.2ms\n",
            "Speed: 3.6ms preprocess, 139.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 158.5ms\n",
            "Speed: 3.4ms preprocess, 158.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 143.9ms\n",
            "Speed: 3.5ms preprocess, 143.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 131.5ms\n",
            "Speed: 8.0ms preprocess, 131.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 141.7ms\n",
            "Speed: 7.2ms preprocess, 141.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 126.1ms\n",
            "Speed: 3.8ms preprocess, 126.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 126.6ms\n",
            "Speed: 2.9ms preprocess, 126.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 154.4ms\n",
            "Speed: 4.3ms preprocess, 154.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.7ms\n",
            "Speed: 3.4ms preprocess, 133.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 142.1ms\n",
            "Speed: 3.4ms preprocess, 142.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 135.2ms\n",
            "Speed: 6.3ms preprocess, 135.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.2ms\n",
            "Speed: 3.3ms preprocess, 133.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 151.5ms\n",
            "Speed: 3.2ms preprocess, 151.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 152.2ms\n",
            "Speed: 6.2ms preprocess, 152.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.4ms\n",
            "Speed: 3.5ms preprocess, 136.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 146.7ms\n",
            "Speed: 3.4ms preprocess, 146.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 146.0ms\n",
            "Speed: 3.4ms preprocess, 146.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 138.3ms\n",
            "Speed: 3.3ms preprocess, 138.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 130.2ms\n",
            "Speed: 3.5ms preprocess, 130.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 145.6ms\n",
            "Speed: 3.9ms preprocess, 145.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 129.8ms\n",
            "Speed: 7.1ms preprocess, 129.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 132.3ms\n",
            "Speed: 3.2ms preprocess, 132.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 161.2ms\n",
            "Speed: 3.4ms preprocess, 161.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 131.7ms\n",
            "Speed: 3.5ms preprocess, 131.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 126.4ms\n",
            "Speed: 3.3ms preprocess, 126.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 141.1ms\n",
            "Speed: 8.5ms preprocess, 141.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 135.3ms\n",
            "Speed: 3.3ms preprocess, 135.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 133.0ms\n",
            "Speed: 5.6ms preprocess, 133.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 chair, 1 bed, 229.7ms\n",
            "Speed: 8.6ms preprocess, 229.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 chair, 1 bed, 203.6ms\n",
            "Speed: 6.8ms preprocess, 203.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 chair, 1 bed, 222.9ms\n",
            "Speed: 3.2ms preprocess, 222.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 chair, 1 bed, 223.6ms\n",
            "Speed: 3.4ms preprocess, 223.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 1 sink, 228.5ms\n",
            "Speed: 3.2ms preprocess, 228.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 213.7ms\n",
            "Speed: 3.4ms preprocess, 213.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 221.8ms\n",
            "Speed: 3.4ms preprocess, 221.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\n",
            "0: 384x640 1 person, 1 bed, 223.4ms\n",
            "Speed: 3.4ms preprocess, 223.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 211.1ms\n",
            "Speed: 3.3ms preprocess, 211.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 202.6ms\n",
            "Speed: 3.4ms preprocess, 202.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 146.4ms\n",
            "Speed: 5.5ms preprocess, 146.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.8ms\n",
            "Speed: 8.1ms preprocess, 139.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 134.0ms\n",
            "Speed: 8.6ms preprocess, 134.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 143.7ms\n",
            "Speed: 7.1ms preprocess, 143.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 149.1ms\n",
            "Speed: 6.7ms preprocess, 149.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 130.2ms\n",
            "Speed: 4.9ms preprocess, 130.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.0ms\n",
            "Speed: 4.0ms preprocess, 136.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 4 beds, 147.3ms\n",
            "Speed: 5.4ms preprocess, 147.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 133.4ms\n",
            "Speed: 3.4ms preprocess, 133.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 140.8ms\n",
            "Speed: 3.8ms preprocess, 140.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 128.3ms\n",
            "Speed: 5.3ms preprocess, 128.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.7ms\n",
            "Speed: 3.3ms preprocess, 129.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 140.4ms\n",
            "Speed: 3.5ms preprocess, 140.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 140.4ms\n",
            "Speed: 5.6ms preprocess, 140.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.2ms\n",
            "Speed: 9.0ms preprocess, 129.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 134.8ms\n",
            "Speed: 3.4ms preprocess, 134.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 146.0ms\n",
            "Speed: 4.2ms preprocess, 146.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 132.8ms\n",
            "Speed: 4.6ms preprocess, 132.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 135.5ms\n",
            "Speed: 10.8ms preprocess, 135.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 140.2ms\n",
            "Speed: 3.3ms preprocess, 140.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 145.4ms\n",
            "Speed: 3.4ms preprocess, 145.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.9ms\n",
            "Speed: 3.2ms preprocess, 133.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 135.2ms\n",
            "Speed: 7.1ms preprocess, 135.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 150.1ms\n",
            "Speed: 3.3ms preprocess, 150.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 127.8ms\n",
            "Speed: 3.3ms preprocess, 127.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 141.6ms\n",
            "Speed: 3.3ms preprocess, 141.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 152.8ms\n",
            "Speed: 7.0ms preprocess, 152.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 132.6ms\n",
            "Speed: 3.3ms preprocess, 132.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 139.2ms\n",
            "Speed: 3.6ms preprocess, 139.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 144.9ms\n",
            "Speed: 3.4ms preprocess, 144.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 131.8ms\n",
            "Speed: 3.9ms preprocess, 131.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 219.2ms\n",
            "Speed: 6.2ms preprocess, 219.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 229.5ms\n",
            "Speed: 3.4ms preprocess, 229.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 214.2ms\n",
            "Speed: 5.4ms preprocess, 214.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 218.5ms\n",
            "Speed: 4.5ms preprocess, 218.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 215.8ms\n",
            "Speed: 4.0ms preprocess, 215.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 208.2ms\n",
            "Speed: 7.1ms preprocess, 208.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 225.0ms\n",
            "Speed: 3.3ms preprocess, 225.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 218.8ms\n",
            "Speed: 3.3ms preprocess, 218.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 252.5ms\n",
            "Speed: 8.4ms preprocess, 252.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 227.1ms\n",
            "Speed: 3.3ms preprocess, 227.1ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 141.1ms\n",
            "Speed: 3.2ms preprocess, 141.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.4ms\n",
            "Speed: 5.2ms preprocess, 133.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.6ms\n",
            "Speed: 3.3ms preprocess, 129.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 141.2ms\n",
            "Speed: 3.2ms preprocess, 141.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.9ms\n",
            "Speed: 3.5ms preprocess, 136.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 138.1ms\n",
            "Speed: 3.5ms preprocess, 138.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 160.7ms\n",
            "Speed: 7.8ms preprocess, 160.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 384x640 1 person, 3 beds, 138.2ms\n",
            "Speed: 4.7ms preprocess, 138.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.5ms\n",
            "Speed: 3.8ms preprocess, 136.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.9ms\n",
            "Speed: 3.6ms preprocess, 139.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.2ms\n",
            "Speed: 3.8ms preprocess, 136.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 152.2ms\n",
            "Speed: 7.8ms preprocess, 152.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 140.7ms\n",
            "Speed: 3.4ms preprocess, 140.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 138.5ms\n",
            "Speed: 3.4ms preprocess, 138.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 130.1ms\n",
            "Speed: 3.4ms preprocess, 130.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 137.8ms\n",
            "Speed: 5.2ms preprocess, 137.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 136.1ms\n",
            "Speed: 3.6ms preprocess, 136.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 147.1ms\n",
            "Speed: 3.5ms preprocess, 147.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 144.0ms\n",
            "Speed: 3.0ms preprocess, 144.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 126.5ms\n",
            "Speed: 3.2ms preprocess, 126.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 151.5ms\n",
            "Speed: 3.7ms preprocess, 151.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 143.6ms\n",
            "Speed: 4.8ms preprocess, 143.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 128.9ms\n",
            "Speed: 2.8ms preprocess, 128.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.3ms\n",
            "Speed: 2.8ms preprocess, 129.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.4ms\n",
            "Speed: 2.8ms preprocess, 139.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 123.8ms\n",
            "Speed: 2.9ms preprocess, 123.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 133.9ms\n",
            "Speed: 2.8ms preprocess, 133.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 142.5ms\n",
            "Speed: 4.0ms preprocess, 142.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 139.8ms\n",
            "Speed: 2.8ms preprocess, 139.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 129.6ms\n",
            "Speed: 2.9ms preprocess, 129.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 121.5ms\n",
            "Speed: 3.4ms preprocess, 121.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 221.8ms\n",
            "Speed: 2.9ms preprocess, 221.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\n",
            "0: 384x640 1 person, 2 beds, 202.0ms\n",
            "Speed: 2.7ms preprocess, 202.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install tensorflow opencv-python-headless matplotlib ultralytics\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, TimeDistributed,\n",
        "    BatchNormalization, Attention, Input\n",
        ")\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Step 1: Define the CNN-LSTM model with attention mechanism\n",
        "def build_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # CNN layers\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(inputs)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(256, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention_out = Attention()([x, x])\n",
        "\n",
        "    # LSTM layers\n",
        "    lstm_out = LSTM(128, return_sequences=True)(attention_out)\n",
        "    lstm_out = LSTM(64)(lstm_out)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dense(128, activation='relu')(lstm_out)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 2: Data preprocessing\n",
        "def preprocess_frame(frame, target_size=(64, 64)):\n",
        "    frame = cv2.resize(frame, target_size)\n",
        "    return frame / 255.0  # Normalize pixel values\n",
        "\n",
        "def load_video_data(path, label, sequence_length):\n",
        "    sequences, labels = [], []\n",
        "    for video_path in glob(os.path.join(path, \"*.mp4\")):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while len(frames) < sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(preprocess_frame(frame))\n",
        "        cap.release()\n",
        "        if len(frames) == sequence_length:\n",
        "            sequences.append(frames)\n",
        "            labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Step 3: Load datasets\n",
        "def load_datasets(train_paths, val_paths, sequence_length):\n",
        "    X_train, y_train, X_val, y_val = [], [], [], []\n",
        "\n",
        "    for path, label in train_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length)\n",
        "        X_train.append(X)\n",
        "        y_train.append(y)\n",
        "\n",
        "    for path, label in val_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length)\n",
        "        X_val.append(X)\n",
        "        y_val.append(y)\n",
        "\n",
        "    X_train, y_train = np.concatenate(X_train), np.concatenate(y_train)\n",
        "    X_val, y_val = np.concatenate(X_val), np.concatenate(y_val)\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val)\n",
        "\n",
        "# Define paths for training and validation datasets\n",
        "train_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/nofall\", 0)\n",
        "]\n",
        "val_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/nofall\", 0)\n",
        "]\n",
        "sequence_length = 10  # Number of frames in a sequence\n",
        "\n",
        "# Load the datasets\n",
        "(X_train, y_train), (X_val, y_val) = load_datasets(train_paths, val_paths, sequence_length)\n",
        "\n",
        "# Shuffle the datasets\n",
        "train_indices = np.random.permutation(len(X_train))\n",
        "X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
        "val_indices = np.random.permutation(len(X_val))\n",
        "X_val, y_val = X_val[val_indices], y_val[val_indices]\n",
        "\n",
        "# Step 4: Train the model\n",
        "input_shape = (sequence_length, 64, 64, 3)\n",
        "model = build_model(input_shape)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=4,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/fall_detection_model.keras')\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "def evaluate_model(test_paths, model, sequence_length):\n",
        "    X_test, y_test = [], []\n",
        "    for path, label in test_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length)\n",
        "        X_test.append(X)\n",
        "        y_test.append(y)\n",
        "    X_test, y_test = np.concatenate(X_test), np.concatenate(y_test)\n",
        "    indices = np.random.permutation(len(X_test))\n",
        "    X_test, y_test = X_test[indices], y_test[indices]\n",
        "    return model.evaluate(X_test, y_test)\n",
        "\n",
        "test_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/nofall\", 0)\n",
        "]\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(test_paths, model, sequence_length)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Step 6: Visualize training history\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# Step 7: Fall detection function\n",
        "def detect_falls(model, input_path, output_path, sequence_length):\n",
        "    \"\"\"\n",
        "    Detect falls in a video using a trained model and YOLO for person detection.\n",
        "\n",
        "    Args:\n",
        "        model: Trained TensorFlow model for fall detection.\n",
        "        input_path: Path to the input video.\n",
        "        output_path: Path to save the output video with detections.\n",
        "        sequence_length: Number of frames in a sequence for fall detection.\n",
        "    \"\"\"\n",
        "    # Load the video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Unable to open video {input_path}\")\n",
        "        return\n",
        "\n",
        "    # Video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Output video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Load YOLOv8 model\n",
        "    yolo_model = YOLO('yolov8n.pt')  # Lightweight YOLOv8 model\n",
        "\n",
        "    # Sequence buffer for fall detection\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Use YOLOv8 to detect persons\n",
        "        results = yolo_model(frame)\n",
        "        detections = results[0].boxes\n",
        "\n",
        "        for detection in detections:\n",
        "            if detection.cls == 0:  # Class 0 corresponds to 'person'\n",
        "                # Extract bounding box coordinates\n",
        "                coords = detection.xyxy.cpu().numpy().astype(int)[0]\n",
        "                x1, y1, x2, y2 = coords\n",
        "\n",
        "                # Draw bounding box\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "                # Extract and preprocess the detected person\n",
        "                person_frame = frame[y1:y2, x1:x2]\n",
        "                processed_frame = preprocess_frame(person_frame)\n",
        "                frames.append(processed_frame)\n",
        "\n",
        "                # If enough frames are collected, predict fall\n",
        "                if len(frames) == sequence_length:\n",
        "                    # Prepare the sequence for prediction\n",
        "                    frames_sequence = np.array(frames).reshape(1, sequence_length, 64, 64, 3)\n",
        "                    prediction = model.predict(frames_sequence)\n",
        "\n",
        "                    # Check if the prediction indicates a fall\n",
        "                    if prediction >= 0.55:\n",
        "                        cv2.putText(frame, \"Fall Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "                    frames.pop(0)  # Remove the oldest frame to maintain the sequence length\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Step 8: Apply fall detection to an input video\n",
        "input_video = '/content/drive/MyDrive/test.mp4'\n",
        "output_video = '/content/drive/MyDrive/fall_detection_output.mp4'\n",
        "sequence_length = 10  # Number of frames for sequence\n",
        "\n",
        "detect_falls(model, input_video, output_video, sequence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9UlA4FF7WcV",
        "outputId": "3d9a7434-a5ce-44b6-f855-9b42d8a1d7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 145.8ms\n",
            "Speed: 3.2ms preprocess, 145.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.5ms\n",
            "Speed: 2.9ms preprocess, 135.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 146.5ms\n",
            "Speed: 2.9ms preprocess, 146.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 148.7ms\n",
            "Speed: 3.8ms preprocess, 148.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 133.7ms\n",
            "Speed: 4.1ms preprocess, 133.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.2ms\n",
            "Speed: 4.1ms preprocess, 138.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.5ms\n",
            "Speed: 3.2ms preprocess, 137.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.8ms\n",
            "Speed: 4.1ms preprocess, 134.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.2ms\n",
            "Speed: 3.7ms preprocess, 135.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 145.6ms\n",
            "Speed: 3.8ms preprocess, 145.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 142.1ms\n",
            "Speed: 4.5ms preprocess, 142.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 2 chairs, 1 tv, 142.3ms\n",
            "Speed: 3.1ms preprocess, 142.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 chairs, 1 tv, 136.2ms\n",
            "Speed: 4.1ms preprocess, 136.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 150.6ms\n",
            "Speed: 6.2ms preprocess, 150.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 142.5ms\n",
            "Speed: 3.2ms preprocess, 142.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 227.4ms\n",
            "Speed: 3.6ms preprocess, 227.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 261.2ms\n",
            "Speed: 4.3ms preprocess, 261.2ms inference, 8.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 223.4ms\n",
            "Speed: 3.8ms preprocess, 223.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 214.6ms\n",
            "Speed: 5.9ms preprocess, 214.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 221.8ms\n",
            "Speed: 3.8ms preprocess, 221.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 223.4ms\n",
            "Speed: 3.3ms preprocess, 223.4ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 224.5ms\n",
            "Speed: 3.5ms preprocess, 224.5ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 263.4ms\n",
            "Speed: 3.5ms preprocess, 263.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 225.9ms\n",
            "Speed: 5.3ms preprocess, 225.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 178.1ms\n",
            "Speed: 3.8ms preprocess, 178.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 dog, 1 chair, 1 tv, 153.0ms\n",
            "Speed: 4.0ms preprocess, 153.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.2ms\n",
            "Speed: 3.5ms preprocess, 134.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.2ms\n",
            "Speed: 3.6ms preprocess, 136.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 146.7ms\n",
            "Speed: 4.2ms preprocess, 146.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 147.0ms\n",
            "Speed: 4.9ms preprocess, 147.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.1ms\n",
            "Speed: 3.9ms preprocess, 138.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.8ms\n",
            "Speed: 3.9ms preprocess, 135.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 145.4ms\n",
            "Speed: 4.8ms preprocess, 145.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.1ms\n",
            "Speed: 3.3ms preprocess, 136.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.6ms\n",
            "Speed: 4.0ms preprocess, 139.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 155.0ms\n",
            "Speed: 3.5ms preprocess, 155.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 156.3ms\n",
            "Speed: 4.1ms preprocess, 156.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 140.1ms\n",
            "Speed: 4.4ms preprocess, 140.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 140.0ms\n",
            "Speed: 4.3ms preprocess, 140.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.0ms\n",
            "Speed: 5.1ms preprocess, 139.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.4ms\n",
            "Speed: 4.1ms preprocess, 137.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 144.6ms\n",
            "Speed: 4.8ms preprocess, 144.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 147.0ms\n",
            "Speed: 5.0ms preprocess, 147.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 149.7ms\n",
            "Speed: 2.6ms preprocess, 149.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 138.1ms\n",
            "Speed: 4.8ms preprocess, 138.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 145.7ms\n",
            "Speed: 4.3ms preprocess, 145.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 163.9ms\n",
            "Speed: 4.4ms preprocess, 163.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 133.1ms\n",
            "Speed: 3.9ms preprocess, 133.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 152.6ms\n",
            "Speed: 3.2ms preprocess, 152.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 133.6ms\n",
            "Speed: 4.3ms preprocess, 133.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 144.8ms\n",
            "Speed: 4.1ms preprocess, 144.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 150.0ms\n",
            "Speed: 4.3ms preprocess, 150.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 135.5ms\n",
            "Speed: 4.5ms preprocess, 135.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 133.9ms\n",
            "Speed: 4.0ms preprocess, 133.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.6ms\n",
            "Speed: 4.1ms preprocess, 137.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 147.6ms\n",
            "Speed: 4.4ms preprocess, 147.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.4ms\n",
            "Speed: 4.2ms preprocess, 137.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 227.0ms\n",
            "Speed: 5.9ms preprocess, 227.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 232.8ms\n",
            "Speed: 3.8ms preprocess, 232.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 227.4ms\n",
            "Speed: 4.7ms preprocess, 227.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 228.5ms\n",
            "Speed: 3.6ms preprocess, 228.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 232.0ms\n",
            "Speed: 3.8ms preprocess, 232.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 223.2ms\n",
            "Speed: 3.7ms preprocess, 223.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 220.7ms\n",
            "Speed: 3.7ms preprocess, 220.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 217.1ms\n",
            "Speed: 3.7ms preprocess, 217.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 219.8ms\n",
            "Speed: 3.7ms preprocess, 219.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 1 keyboard, 223.3ms\n",
            "Speed: 3.6ms preprocess, 223.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 1 keyboard, 229.5ms\n",
            "Speed: 3.7ms preprocess, 229.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 1 keyboard, 231.5ms\n",
            "Speed: 3.5ms preprocess, 231.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 234.4ms\n",
            "Speed: 4.2ms preprocess, 234.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 448x640 1 cat, 1 chair, 2 tvs, 243.1ms\n",
            "Speed: 4.6ms preprocess, 243.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 228.4ms\n",
            "Speed: 3.8ms preprocess, 228.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 251.8ms\n",
            "Speed: 3.7ms preprocess, 251.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 132.8ms\n",
            "Speed: 4.6ms preprocess, 132.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 134.6ms\n",
            "Speed: 4.3ms preprocess, 134.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 138.3ms\n",
            "Speed: 3.4ms preprocess, 138.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 cat, 1 chair, 2 tvs, 146.1ms\n",
            "Speed: 3.1ms preprocess, 146.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 151.5ms\n",
            "Speed: 3.5ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 dog, 1 chair, 2 tvs, 147.5ms\n",
            "Speed: 4.7ms preprocess, 147.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 139.1ms\n",
            "Speed: 3.7ms preprocess, 139.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 136.9ms\n",
            "Speed: 5.5ms preprocess, 136.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 146.5ms\n",
            "Speed: 4.1ms preprocess, 146.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 cat, 1 chair, 2 tvs, 139.1ms\n",
            "Speed: 3.5ms preprocess, 139.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 152.4ms\n",
            "Speed: 3.5ms preprocess, 152.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 dog, 1 chair, 2 tvs, 154.8ms\n",
            "Speed: 3.8ms preprocess, 154.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.4ms\n",
            "Speed: 3.8ms preprocess, 136.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.9ms\n",
            "Speed: 4.2ms preprocess, 139.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 153.3ms\n",
            "Speed: 5.1ms preprocess, 153.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 140.9ms\n",
            "Speed: 5.0ms preprocess, 140.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 135.5ms\n",
            "Speed: 4.5ms preprocess, 135.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 3 persons, 1 chair, 1 tv, 162.4ms\n",
            "Speed: 3.6ms preprocess, 162.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 3 persons, 1 chair, 1 tv, 139.7ms\n",
            "Speed: 4.2ms preprocess, 139.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 137.2ms\n",
            "Speed: 6.2ms preprocess, 137.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.5ms\n",
            "Speed: 3.4ms preprocess, 135.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.5ms\n",
            "Speed: 4.2ms preprocess, 136.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 140.8ms\n",
            "Speed: 3.5ms preprocess, 140.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 140.1ms\n",
            "Speed: 3.7ms preprocess, 140.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 144.5ms\n",
            "Speed: 4.4ms preprocess, 144.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 135.6ms\n",
            "Speed: 4.3ms preprocess, 135.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.9ms\n",
            "Speed: 4.4ms preprocess, 137.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 167.3ms\n",
            "Speed: 3.9ms preprocess, 167.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.6ms\n",
            "Speed: 4.8ms preprocess, 137.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 143.4ms\n",
            "Speed: 3.5ms preprocess, 143.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 149.4ms\n",
            "Speed: 4.5ms preprocess, 149.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.9ms\n",
            "Speed: 3.5ms preprocess, 135.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.2ms\n",
            "Speed: 3.7ms preprocess, 138.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.9ms\n",
            "Speed: 4.0ms preprocess, 138.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 147.0ms\n",
            "Speed: 3.1ms preprocess, 147.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 chair, 2 tvs, 136.3ms\n",
            "Speed: 3.9ms preprocess, 136.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 139.3ms\n",
            "Speed: 4.2ms preprocess, 139.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.8ms\n",
            "Speed: 3.7ms preprocess, 135.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 192.7ms\n",
            "Speed: 3.8ms preprocess, 192.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 215.4ms\n",
            "Speed: 3.6ms preprocess, 215.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 260.2ms\n",
            "Speed: 7.9ms preprocess, 260.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 232.6ms\n",
            "Speed: 3.9ms preprocess, 232.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 221.0ms\n",
            "Speed: 8.4ms preprocess, 221.0ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 222.9ms\n",
            "Speed: 4.4ms preprocess, 222.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 213.4ms\n",
            "Speed: 3.9ms preprocess, 213.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 226.5ms\n",
            "Speed: 6.5ms preprocess, 226.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 216.8ms\n",
            "Speed: 5.1ms preprocess, 216.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 219.8ms\n",
            "Speed: 3.6ms preprocess, 219.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 246.9ms\n",
            "Speed: 3.6ms preprocess, 246.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 215.6ms\n",
            "Speed: 5.0ms preprocess, 215.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 154.6ms\n",
            "Speed: 3.0ms preprocess, 154.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 150.2ms\n",
            "Speed: 3.7ms preprocess, 150.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 140.0ms\n",
            "Speed: 3.5ms preprocess, 140.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 145.8ms\n",
            "Speed: 5.4ms preprocess, 145.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.4ms\n",
            "Speed: 3.8ms preprocess, 135.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.5ms\n",
            "Speed: 4.5ms preprocess, 136.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 142.6ms\n",
            "Speed: 3.5ms preprocess, 142.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.6ms\n",
            "Speed: 3.9ms preprocess, 136.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 151.7ms\n",
            "Speed: 3.6ms preprocess, 151.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.2ms\n",
            "Speed: 4.2ms preprocess, 135.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 146.0ms\n",
            "Speed: 4.9ms preprocess, 146.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.5ms\n",
            "Speed: 4.5ms preprocess, 134.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.9ms\n",
            "Speed: 4.4ms preprocess, 138.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 149.2ms\n",
            "Speed: 5.0ms preprocess, 149.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 133.9ms\n",
            "Speed: 4.2ms preprocess, 133.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 138.2ms\n",
            "Speed: 4.4ms preprocess, 138.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 149.5ms\n",
            "Speed: 4.3ms preprocess, 149.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.7ms\n",
            "Speed: 3.6ms preprocess, 136.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.1ms\n",
            "Speed: 3.5ms preprocess, 134.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 157.9ms\n",
            "Speed: 5.1ms preprocess, 157.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 135.8ms\n",
            "Speed: 3.8ms preprocess, 135.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 142.2ms\n",
            "Speed: 3.7ms preprocess, 142.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 148.3ms\n",
            "Speed: 3.2ms preprocess, 148.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 146.1ms\n",
            "Speed: 3.3ms preprocess, 146.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 149.1ms\n",
            "Speed: 4.5ms preprocess, 149.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 133.3ms\n",
            "Speed: 4.4ms preprocess, 133.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 137.3ms\n",
            "Speed: 4.3ms preprocess, 137.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 151.0ms\n",
            "Speed: 3.8ms preprocess, 151.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 145.6ms\n",
            "Speed: 3.2ms preprocess, 145.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 146.7ms\n",
            "Speed: 4.5ms preprocess, 146.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 193.7ms\n",
            "Speed: 3.8ms preprocess, 193.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 214.5ms\n",
            "Speed: 7.8ms preprocess, 214.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 213.2ms\n",
            "Speed: 7.0ms preprocess, 213.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 222.1ms\n",
            "Speed: 4.5ms preprocess, 222.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 213.6ms\n",
            "Speed: 3.9ms preprocess, 213.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 228.0ms\n",
            "Speed: 7.7ms preprocess, 228.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 218.0ms\n",
            "Speed: 3.8ms preprocess, 218.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 263.9ms\n",
            "Speed: 4.1ms preprocess, 263.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 205.8ms\n",
            "Speed: 3.6ms preprocess, 205.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 168.2ms\n",
            "Speed: 3.8ms preprocess, 168.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.8ms\n",
            "Speed: 4.2ms preprocess, 137.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 146.6ms\n",
            "Speed: 3.8ms preprocess, 146.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.6ms\n",
            "Speed: 3.6ms preprocess, 137.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 2 chairs, 1 tv, 133.2ms\n",
            "Speed: 3.3ms preprocess, 133.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 139.8ms\n",
            "Speed: 3.7ms preprocess, 139.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.3ms\n",
            "Speed: 3.8ms preprocess, 134.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 164.2ms\n",
            "Speed: 4.5ms preprocess, 164.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 141.7ms\n",
            "Speed: 3.5ms preprocess, 141.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.3ms\n",
            "Speed: 3.5ms preprocess, 135.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 156.0ms\n",
            "Speed: 4.1ms preprocess, 156.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 139.7ms\n",
            "Speed: 5.2ms preprocess, 139.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 135.3ms\n",
            "Speed: 3.6ms preprocess, 135.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 145.4ms\n",
            "Speed: 4.6ms preprocess, 145.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 136.7ms\n",
            "Speed: 4.0ms preprocess, 136.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 138.7ms\n",
            "Speed: 3.7ms preprocess, 138.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 2 tvs, 136.3ms\n",
            "Speed: 4.3ms preprocess, 136.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 151.6ms\n",
            "Speed: 3.5ms preprocess, 151.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 134.8ms\n",
            "Speed: 3.9ms preprocess, 134.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.8ms\n",
            "Speed: 3.7ms preprocess, 136.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 155.5ms\n",
            "Speed: 3.6ms preprocess, 155.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 136.6ms\n",
            "Speed: 3.5ms preprocess, 136.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 135.6ms\n",
            "Speed: 3.9ms preprocess, 135.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 2 tvs, 141.1ms\n",
            "Speed: 4.4ms preprocess, 141.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 151.9ms\n",
            "Speed: 5.1ms preprocess, 151.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 139.2ms\n",
            "Speed: 3.7ms preprocess, 139.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 147.8ms\n",
            "Speed: 4.7ms preprocess, 147.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 2 persons, 2 chairs, 1 tv, 135.4ms\n",
            "Speed: 3.9ms preprocess, 135.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 135.0ms\n",
            "Speed: 3.6ms preprocess, 135.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 134.3ms\n",
            "Speed: 3.9ms preprocess, 134.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 2 tvs, 141.6ms\n",
            "Speed: 3.1ms preprocess, 141.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 137.8ms\n",
            "Speed: 3.9ms preprocess, 137.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 2 tvs, 193.2ms\n",
            "Speed: 3.8ms preprocess, 193.2ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 2 chairs, 1 tv, 227.2ms\n",
            "Speed: 7.0ms preprocess, 227.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 217.9ms\n",
            "Speed: 3.9ms preprocess, 217.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 236.3ms\n",
            "Speed: 3.7ms preprocess, 236.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 213.4ms\n",
            "Speed: 3.8ms preprocess, 213.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 246.4ms\n",
            "Speed: 7.9ms preprocess, 246.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 220.0ms\n",
            "Speed: 5.9ms preprocess, 220.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 227.2ms\n",
            "Speed: 3.7ms preprocess, 227.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 252.7ms\n",
            "Speed: 6.6ms preprocess, 252.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 235.7ms\n",
            "Speed: 5.2ms preprocess, 235.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.9ms\n",
            "Speed: 3.6ms preprocess, 137.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 158.1ms\n",
            "Speed: 3.5ms preprocess, 158.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.9ms\n",
            "Speed: 4.7ms preprocess, 136.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 147.2ms\n",
            "Speed: 3.2ms preprocess, 147.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 148.8ms\n",
            "Speed: 3.8ms preprocess, 148.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 141.0ms\n",
            "Speed: 6.3ms preprocess, 141.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.1ms\n",
            "Speed: 4.3ms preprocess, 139.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 136.1ms\n",
            "Speed: 3.5ms preprocess, 136.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 146.0ms\n",
            "Speed: 3.5ms preprocess, 146.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 141.6ms\n",
            "Speed: 3.9ms preprocess, 141.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 147.0ms\n",
            "Speed: 3.8ms preprocess, 147.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 154.7ms\n",
            "Speed: 5.0ms preprocess, 154.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 136.0ms\n",
            "Speed: 3.7ms preprocess, 136.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 139.1ms\n",
            "Speed: 3.6ms preprocess, 139.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 141.8ms\n",
            "Speed: 3.4ms preprocess, 141.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 144.0ms\n",
            "Speed: 3.7ms preprocess, 144.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.4ms\n",
            "Speed: 3.8ms preprocess, 136.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 142.9ms\n",
            "Speed: 4.3ms preprocess, 142.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 139.5ms\n",
            "Speed: 3.8ms preprocess, 139.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.1ms\n",
            "Speed: 4.4ms preprocess, 139.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 141.5ms\n",
            "Speed: 3.7ms preprocess, 141.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 137.7ms\n",
            "Speed: 4.3ms preprocess, 137.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 149.7ms\n",
            "Speed: 8.9ms preprocess, 149.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.9ms\n",
            "Speed: 3.7ms preprocess, 139.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 146.6ms\n",
            "Speed: 4.2ms preprocess, 146.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 139.8ms\n",
            "Speed: 3.9ms preprocess, 139.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 2 tvs, 137.4ms\n",
            "Speed: 3.4ms preprocess, 137.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 tv, 142.7ms\n",
            "Speed: 5.5ms preprocess, 142.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.2ms\n",
            "Speed: 3.5ms preprocess, 139.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 218.0ms\n",
            "Speed: 3.6ms preprocess, 218.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 221.5ms\n",
            "Speed: 4.7ms preprocess, 221.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 230.9ms\n",
            "Speed: 3.8ms preprocess, 230.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 249.4ms\n",
            "Speed: 4.4ms preprocess, 249.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 256.5ms\n",
            "Speed: 3.7ms preprocess, 256.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 212.1ms\n",
            "Speed: 4.4ms preprocess, 212.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 224.9ms\n",
            "Speed: 3.8ms preprocess, 224.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 2 tvs, 250.9ms\n",
            "Speed: 3.6ms preprocess, 250.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 236.3ms\n",
            "Speed: 4.7ms preprocess, 236.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 144.5ms\n",
            "Speed: 3.8ms preprocess, 144.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 142.4ms\n",
            "Speed: 3.9ms preprocess, 142.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 157.9ms\n",
            "Speed: 4.3ms preprocess, 157.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 141.9ms\n",
            "Speed: 4.2ms preprocess, 141.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 140.9ms\n",
            "Speed: 5.2ms preprocess, 140.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 147.7ms\n",
            "Speed: 4.0ms preprocess, 147.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 136.8ms\n",
            "Speed: 4.7ms preprocess, 136.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 137.3ms\n",
            "Speed: 5.4ms preprocess, 137.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 144.7ms\n",
            "Speed: 4.3ms preprocess, 144.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 1 chair, 1 tv, 160.0ms\n",
            "Speed: 4.0ms preprocess, 160.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\n",
            "0: 448x640 2 persons, 2 chairs, 1 tv, 148.9ms\n",
            "Speed: 3.9ms preprocess, 148.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 chairs, 1 tv, 136.8ms\n",
            "Speed: 3.8ms preprocess, 136.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 cat, 2 chairs, 1 tv, 149.6ms\n",
            "Speed: 3.9ms preprocess, 149.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 140.7ms\n",
            "Speed: 3.8ms preprocess, 140.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 137.4ms\n",
            "Speed: 4.0ms preprocess, 137.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 143.1ms\n",
            "Speed: 3.5ms preprocess, 143.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 151.7ms\n",
            "Speed: 3.8ms preprocess, 151.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 142.7ms\n",
            "Speed: 3.9ms preprocess, 142.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 156.7ms\n",
            "Speed: 7.0ms preprocess, 156.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 150.8ms\n",
            "Speed: 3.8ms preprocess, 150.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 141.0ms\n",
            "Speed: 4.0ms preprocess, 141.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 139.0ms\n",
            "Speed: 3.0ms preprocess, 139.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 157.3ms\n",
            "Speed: 3.8ms preprocess, 157.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.3ms\n",
            "Speed: 3.8ms preprocess, 139.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 136.3ms\n",
            "Speed: 5.0ms preprocess, 136.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 148.1ms\n",
            "Speed: 4.3ms preprocess, 148.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 1 chair, 1 tv, 140.6ms\n",
            "Speed: 3.7ms preprocess, 140.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 155.4ms\n",
            "Speed: 4.0ms preprocess, 155.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 137.5ms\n",
            "Speed: 3.9ms preprocess, 137.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 194.0ms\n",
            "Speed: 3.7ms preprocess, 194.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 chair, 1 tv, 220.5ms\n",
            "Speed: 3.5ms preprocess, 220.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 chairs, 1 tv, 223.7ms\n",
            "Speed: 4.2ms preprocess, 223.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 213.8ms\n",
            "Speed: 3.7ms preprocess, 213.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 244.1ms\n",
            "Speed: 7.3ms preprocess, 244.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 244.4ms\n",
            "Speed: 3.8ms preprocess, 244.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\n",
            "0: 448x640 2 persons, 1 chair, 1 tv, 220.3ms\n",
            "Speed: 3.7ms preprocess, 220.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 228.1ms\n",
            "Speed: 3.9ms preprocess, 228.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 1 teddy bear, 241.3ms\n",
            "Speed: 5.9ms preprocess, 241.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 242.5ms\n",
            "Speed: 3.7ms preprocess, 242.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 230.8ms\n",
            "Speed: 3.6ms preprocess, 230.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\n",
            "0: 448x640 1 person, 2 cats, 1 chair, 1 tv, 236.8ms\n",
            "Speed: 4.0ms preprocess, 236.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 142.4ms\n",
            "Speed: 3.5ms preprocess, 142.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 141.0ms\n",
            "Speed: 3.7ms preprocess, 141.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.2ms\n",
            "Speed: 3.7ms preprocess, 139.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 1 chair, 1 tv, 147.5ms\n",
            "Speed: 4.3ms preprocess, 147.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 tv, 139.1ms\n",
            "Speed: 3.8ms preprocess, 139.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n"
          ]
        }
      ],
      "source": [
        "def detect_falls(model, input_path, output_path, sequence_length):\n",
        "    \"\"\"\n",
        "    Detect falls in a video using a trained model and YOLO for person detection.\n",
        "\n",
        "    Args:\n",
        "        model: Trained TensorFlow model for fall detection.\n",
        "        input_path: Path to the input video.\n",
        "        output_path: Path to save the output video with detections.\n",
        "        sequence_length: Number of frames in a sequence for fall detection.\n",
        "    \"\"\"\n",
        "    # Load the video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Unable to open video {input_path}\")\n",
        "        return\n",
        "\n",
        "    # Video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Output video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Load YOLOv8 model\n",
        "    yolo_model = YOLO('yolov8n.pt')  # Lightweight YOLOv8 model\n",
        "\n",
        "    # Sequence buffer for fall detection\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Use YOLOv8 to detect persons\n",
        "        results = yolo_model(frame)\n",
        "        detections = results[0].boxes\n",
        "\n",
        "        for detection in detections:\n",
        "            if detection.cls == 0:  # Class 0 corresponds to 'person'\n",
        "                # Extract bounding box coordinates\n",
        "                coords = detection.xyxy.cpu().numpy().astype(int)[0]\n",
        "                x1, y1, x2, y2 = coords\n",
        "\n",
        "                # Draw bounding box\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "                # Extract and preprocess the detected person\n",
        "                person_frame = frame[y1:y2, x1:x2]\n",
        "                processed_frame = preprocess_frame(person_frame)\n",
        "                frames.append(processed_frame)\n",
        "\n",
        "                # If enough frames are collected, predict fall\n",
        "                if len(frames) == sequence_length:\n",
        "                    # Prepare the sequence for prediction\n",
        "                    frames_sequence = np.array(frames).reshape(1, sequence_length, 64, 64, 3)\n",
        "                    prediction = model.predict(frames_sequence)\n",
        "\n",
        "                    # Check if the prediction indicates a fall\n",
        "                    if prediction >= 0.76669:\n",
        "                        cv2.putText(frame, \"Fall Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "                    frames.pop(0)  # Remove the oldest frame to maintain the sequence length\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# Step 8: Apply fall detection to an input video\n",
        "input_video = '/content/drive/MyDrive/Copy of FallSittingS10.avi'\n",
        "output_video = '/content/drive/MyDrive/fall_detection_output3.avi'\n",
        "sequence_length = 10  # Number of frames for sequence\n",
        "\n",
        "detect_falls(model, input_video, output_video, sequence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKv1UF_T84za",
        "outputId": "0ada0f77-c675-4661-96e1-de8cf77a6f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.38-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading ultralytics-8.3.38-py3-none-any.whl (896 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.3/896.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.38 ultralytics-thop-2.0.12\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 5s/step - accuracy: 0.3716 - loss: 0.7160 - val_accuracy: 0.4815 - val_loss: 0.6873 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4s/step - accuracy: 0.5194 - loss: 0.6958 - val_accuracy: 0.5556 - val_loss: 0.6919 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 4s/step - accuracy: 0.4820 - loss: 0.7204 - val_accuracy: 0.6667 - val_loss: 0.6752 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 5s/step - accuracy: 0.5887 - loss: 0.6667 - val_accuracy: 0.5926 - val_loss: 0.6875 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 4s/step - accuracy: 0.5904 - loss: 0.6860 - val_accuracy: 0.5185 - val_loss: 0.6814 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 4s/step - accuracy: 0.6349 - loss: 0.6839 - val_accuracy: 0.5926 - val_loss: 0.6738 - learning_rate: 5.0000e-05\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 0.4762 - loss: 0.6949\n",
            "Test Loss: 0.6948572993278503\n",
            "Test Accuracy: 0.4761904776096344\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install tensorflow opencv-python-headless matplotlib ultralytics\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    TimeDistributed, LSTM, Dense, Dropout, Attention, Flatten, BatchNormalization, Input\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Step 1: Define the CNN-LSTM model with pretrained layers and attention mechanism\n",
        "def build_model(input_shape, sequence_length):\n",
        "    inputs = Input(shape=(sequence_length,) + input_shape)\n",
        "\n",
        "    # Pretrained base model (MobileNetV2)\n",
        "    base_model = MobileNetV2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
        "    base_model.trainable = False  # Freeze pretrained layers\n",
        "\n",
        "    # Apply base model to each frame in the sequence using TimeDistributed\n",
        "    x = TimeDistributed(base_model)(inputs)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention_out = Attention()([x, x])\n",
        "\n",
        "    # LSTM layers\n",
        "    lstm_out = LSTM(128, return_sequences=True)(attention_out)\n",
        "    lstm_out = LSTM(64)(lstm_out)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dense(128, activation='relu')(lstm_out)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 2: Data preprocessing\n",
        "def preprocess_frame(frame, target_size=(224, 224)):\n",
        "    frame = cv2.resize(frame, target_size)\n",
        "    return frame / 255.0  # Normalize pixel values\n",
        "\n",
        "def load_video_data(path, label, sequence_length, input_size):\n",
        "    sequences, labels = [], []\n",
        "    for video_path in glob(os.path.join(path, \"*.mp4\")):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while len(frames) < sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(preprocess_frame(frame, target_size=input_size[:2]))\n",
        "        cap.release()\n",
        "        if len(frames) == sequence_length:\n",
        "            sequences.append(frames)\n",
        "            labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Step 3: Load datasets\n",
        "def load_datasets(train_paths, val_paths, sequence_length, input_size):\n",
        "    X_train, y_train, X_val, y_val = [], [], [], []\n",
        "\n",
        "    for path, label in train_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_train.append(X)\n",
        "        y_train.append(y)\n",
        "\n",
        "    for path, label in val_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_val.append(X)\n",
        "        y_val.append(y)\n",
        "\n",
        "    X_train, y_train = np.concatenate(X_train), np.concatenate(y_train)\n",
        "    X_val, y_val = np.concatenate(X_val), np.concatenate(y_val)\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val)\n",
        "\n",
        "# Define paths for training and validation datasets\n",
        "train_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/nofall\", 0)\n",
        "]\n",
        "val_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/nofall\", 0)\n",
        "]\n",
        "sequence_length = 10  # Number of frames in a sequence\n",
        "input_size = (224, 224, 3)\n",
        "\n",
        "# Load the datasets\n",
        "(X_train, y_train), (X_val, y_val) = load_datasets(train_paths, val_paths, sequence_length, input_size)\n",
        "\n",
        "# Shuffle the datasets\n",
        "train_indices = np.random.permutation(len(X_train))\n",
        "X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
        "val_indices = np.random.permutation(len(X_val))\n",
        "X_val, y_val = X_val[val_indices], y_val[val_indices]\n",
        "\n",
        "# Step 4: Train the model\n",
        "input_shape = input_size\n",
        "model = build_model(input_shape, sequence_length)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=4,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/fall_detection_model_with_pretrained.keras')\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "def evaluate_model(test_paths, model, sequence_length, input_size):\n",
        "    X_test, y_test = [], []\n",
        "    for path, label in test_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_test.append(X)\n",
        "        y_test.append(y)\n",
        "    X_test, y_test = np.concatenate(X_test), np.concatenate(y_test)\n",
        "    indices = np.random.permutation(len(X_test))\n",
        "    X_test, y_test = X_test[indices], y_test[indices]\n",
        "    return model.evaluate(X_test, y_test)\n",
        "\n",
        "test_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/nofall\", 0)\n",
        "]\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(test_paths, model, sequence_length, input_size)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "fSpfVjSL0RIb",
        "outputId": "dcb42400-9eca-4c58-f53e-e684d337061a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Conv2D' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-21a9ac0581fe>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Step 4: Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m callbacks = [\n",
            "\u001b[0;32m<ipython-input-7-21a9ac0581fe>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_shape, sequence_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Custom CNN Layers after EfficientNetB0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Conv2D' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 1: Define the CNN-LSTM model with pretrained layers (EfficientNetB0) and attention mechanism\n",
        "def build_model(input_shape, sequence_length):\n",
        "    inputs = Input(shape=(sequence_length,) + input_shape)  # Input shape for video frames\n",
        "\n",
        "    # Pretrained base model (EfficientNetB0 for feature extraction)\n",
        "    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
        "    base_model.trainable = False  # Freeze the pretrained layers for feature extraction\n",
        "\n",
        "    # Apply base model to each frame in the sequence using TimeDistributed\n",
        "    x = TimeDistributed(base_model)(inputs)\n",
        "    x = TimeDistributed(Flatten())(x)  # Flatten the output for further processing\n",
        "\n",
        "    # Custom CNN Layers after EfficientNetB0\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(256, (3, 3), activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    # Flatten the output for LSTM\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Attention mechanism to focus on important frames\n",
        "    attention_out = Attention()([x, x])\n",
        "\n",
        "    # LSTM layers for temporal sequence processing\n",
        "    lstm_out = LSTM(128, return_sequences=True)(attention_out)\n",
        "    lstm_out = LSTM(64)(lstm_out)  # Second LSTM layer for encoding the sequence\n",
        "\n",
        "    # Fully connected layers (Dense)\n",
        "    x = Dense(128, activation='relu')(lstm_out)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer for binary classification (fall or no fall)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 2: Data preprocessing for video frames\n",
        "def preprocess_frame(frame, target_size=(224, 224)):\n",
        "    frame = cv2.resize(frame, target_size)  # Resize frame to target size\n",
        "    return frame / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "def load_video_data(path, label, sequence_length, input_size):\n",
        "    sequences, labels = [], []\n",
        "    for video_path in glob(os.path.join(path, \"*.mp4\")):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while len(frames) < sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(preprocess_frame(frame, target_size=input_size[:2]))\n",
        "        cap.release()\n",
        "        if len(frames) == sequence_length:\n",
        "            sequences.append(frames)\n",
        "            labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Step 3: Load datasets for training and validation\n",
        "def load_datasets(train_paths, val_paths, sequence_length, input_size):\n",
        "    X_train, y_train, X_val, y_val = [], [], [], []\n",
        "\n",
        "    for path, label in train_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_train.append(X)\n",
        "        y_train.append(y)\n",
        "\n",
        "    for path, label in val_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_val.append(X)\n",
        "        y_val.append(y)\n",
        "\n",
        "    X_train, y_train = np.concatenate(X_train), np.concatenate(y_train)\n",
        "    X_val, y_val = np.concatenate(X_val), np.concatenate(y_val)\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val)\n",
        "\n",
        "# Define paths for training and validation datasets\n",
        "train_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/train/nofall\", 0)\n",
        "]\n",
        "val_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/val/nofall\", 0)\n",
        "]\n",
        "sequence_length = 10  # Number of frames in a sequence\n",
        "input_size = (224, 224, 3)  # Input size for frames\n",
        "\n",
        "# Load the datasets\n",
        "(X_train, y_train), (X_val, y_val) = load_datasets(train_paths, val_paths, sequence_length, input_size)\n",
        "\n",
        "# Shuffle the datasets\n",
        "train_indices = np.random.permutation(len(X_train))\n",
        "X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
        "val_indices = np.random.permutation(len(X_val))\n",
        "X_val, y_val = X_val[val_indices], y_val[val_indices]\n",
        "\n",
        "# Step 4: Train the model\n",
        "input_shape = input_size\n",
        "model = build_model(input_shape, sequence_length)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=4,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/fall_detection_model_with_custom_layers.keras')\n",
        "\n",
        "# Step 5: Plot training and validation accuracy/loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "def evaluate_model(test_paths, model, sequence_length, input_size):\n",
        "    X_test, y_test = [], []\n",
        "    for path, label in test_paths:\n",
        "        X, y = load_video_data(path, label, sequence_length, input_size)\n",
        "        X_test.append(X)\n",
        "        y_test.append(y)\n",
        "    X_test, y_test = np.concatenate(X_test), np.concatenate(y_test)\n",
        "    indices = np.random.permutation(len(X_test))\n",
        "    X_test, y_test = X_test[indices], y_test[indices]\n",
        "    return model.evaluate(X_test, y_test)\n",
        "\n",
        "test_paths = [\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/fall\", 1),\n",
        "    (\"/content/drive/MyDrive/datasetvideo/test/nofall\", 0)\n",
        "]\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(test_paths, model, sequence_length, input_size)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1l6wsjW75TU0Mitngxu6Gq4Iz97RZSEFT",
      "authorship_tag": "ABX9TyNvTNUQPgJkmV4LSNDrVZlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}